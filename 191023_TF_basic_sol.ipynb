{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "191023_TF_basic_sol.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Cloud for Big Data",
      "language": "python",
      "name": "bdc"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangqkim/DS2/blob/master/191023_TF_basic_sol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ki_RHIwPJvyn"
      },
      "source": [
        "# 1. TensorFlow Ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s97fwEOoiXk3"
      },
      "source": [
        "## Graph and Session\n",
        "**Graph** : It contains a set of Operations (units of computation) and Tensors (data flow between operations).\n",
        "\n",
        "**Session** : It encapsulates an execution environment such as which operations are executed and what is the current values of Tensor objects. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8p7tgEPijeSd"
      },
      "source": [
        "Let's learn Graph and Session using examples presented below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5N1npMVQqJz"
      },
      "source": [
        "## Constant Op\n",
        "\n",
        "Let's create a constant in TensorFlow.\n",
        "\n",
        "**```tf.constant(value, dtype = None, shape = None, name = 'Const', verify_shape = False)```**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EFZ3bfVsQz_p",
        "outputId": "3b1de250-1d16-4875-c588-4065dffb2ccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # constant of 1d tensor, or a vector\n",
        "  a = tf.constant([2,2], name = 'vector')\n",
        "\n",
        "  # constant of 2x2 tensor, or a matrix\n",
        "  b = tf.constant([[0,2], [1,3]], name = 'matrix')\n",
        "\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"vector:0\", shape=(2,), dtype=int32)\n",
            "Tensor(\"matrix:0\", shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tqBA4_VcXCBe",
        "outputId": "37d9e722-1414-4abc-82f0-3276ce19ce25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Get values of a and b\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  print('a: ')\n",
        "  print(sess.run(a))\n",
        "  print('\\nb:')\n",
        "  print(sess.run(b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: \n",
            "[2 2]\n",
            "\n",
            "b:\n",
            "[[0 2]\n",
            " [1 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xrE4WkZNUn9o"
      },
      "source": [
        "## Math Ops\n",
        "The following example shows a matrix division op."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "drceRvn4VGec",
        "outputId": "25fd990e-a592-41bd-a44c-c7787763548a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # Create constant a and b\n",
        "  a = tf.constant([2,4], name = 'a', dtype = tf.float32)\n",
        "  b = tf.constant([[0,1], [2,3]], name = 'b', dtype = tf.float32)\n",
        "  \n",
        "  # Create divide operation using b and a\n",
        "  div = tf.div(b, a)\n",
        "  # or equivalently, div = b / a\n",
        "  # div = b / a\n",
        "\n",
        "print('Print information of div op')\n",
        "print(div.op)\n",
        "\n",
        "print('\\nPrint div tensor')\n",
        "print(div)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-f52495e6d082>:8: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Print information of div op\n",
            "name: \"div\"\n",
            "op: \"RealDiv\"\n",
            "input: \"b\"\n",
            "input: \"a\"\n",
            "attr {\n",
            "  key: \"T\"\n",
            "  value {\n",
            "    type: DT_FLOAT\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "Print div tensor\n",
            "Tensor(\"div:0\", shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jthhyswKkisO"
      },
      "source": [
        "Run div operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kkmWJN_pY8Xf",
        "outputId": "8559e2ed-3c91-4f15-bb1f-bd904111a685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  print('\\nPrint div')\n",
        "  print(sess.run(div))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Print div\n",
            "[[0.   0.25]\n",
            " [1.   0.75]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cnNhG69dgWsA"
      },
      "source": [
        "## Quiz 1\n",
        "**Define a graph with two constants with shape=[2, 2] and a matmul operation. Print the values of c.\n",
        "(X: matrix multiplication, HINT: use tf.matmul) **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QZyw665-gWsE",
        "outputId": "9e4fa714-2790-4cc2-a435-10067f7181cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ############# Write here. ################\n",
        "  a = tf.constant([[1, 2], [3, 4]])\n",
        "  b = tf.constant([[1, -1], [1, 1]])\n",
        "  c = tf.matmul(a, b)\n",
        "  ##########################################\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  print(sess.run(c))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3 1]\n",
            " [7 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a4UKKwwsZa4I"
      },
      "source": [
        "## Variables\n",
        "\n",
        "TensorFlow object to store mutable state (e.g., model parameters) across multiple session runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2TubsFPeZ0Rz"
      },
      "source": [
        "### Creating variables\n",
        "\n",
        "To declare a variable, you create an instance of the class tf.Variable. tf.constant is written as lowercase because it's an op, and tf.Variable is written with a capital \"V\" because it encapsulates multiple ops.\n",
        "\n",
        "#### Usage of TF Variables\n",
        "\n",
        "\n",
        "```\n",
        "x = tf.Variable(...)\n",
        "x               # variable op\n",
        "x.initializer   # initialization ops\n",
        "x.value         # read op\n",
        "x.assign(...)   # write op\n",
        "x.assign_add(...) # x += ...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zcriAnGbaCKk"
      },
      "source": [
        "One way to create a variable is: \n",
        "\n",
        "**```tf.Variable(< initial-value >, name = < optional-name >)```**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DsaTk0M3hr0p"
      },
      "source": [
        "This example creates three variables using `tf.Variable`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "32t-sxgsaFXh",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # Create scalar variable\n",
        "  s = tf.Variable(2, name = 'scalar')\n",
        "  # Create matrix variable\n",
        "  m = tf.Variable([[0,1], [2,3]], name = 'matrix')\n",
        "  # Create zero matrix using tf.zeros\n",
        "  W = tf.Variable(tf.zeros([784,10]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O91x5GeZzW2N",
        "colab": {}
      },
      "source": [
        "# with tf.Session(graph=graph) as sess:\n",
        "#   print('s:')\n",
        "#   print(sess.run(s))\n",
        "#   print('\\nm:')\n",
        "#   print(sess.run(m))\n",
        "#   print('\\nW:')\n",
        "#   print(sess.run(W))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r9Xymzuia0j3"
      },
      "source": [
        "### Initialize variables\n",
        "\n",
        "Before using a variable, you must initialize it, or else you'll run into an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZpN5DVYmR-U",
        "outputId": "0c0774f5-ecd4-46bc-d56e-067be10617b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  # Run initialization op\n",
        "  sess.run(s.initializer)\n",
        "  sess.run(m.initializer)\n",
        "  sess.run(W.initializer)\n",
        "\n",
        "  print('s:')\n",
        "  print(sess.run(s))\n",
        "  print('\\nm:')\n",
        "  print(sess.run(m))\n",
        "  print('\\nW:')\n",
        "  print(sess.run(W))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s:\n",
            "2\n",
            "\n",
            "m:\n",
            "[[0 1]\n",
            " [2 3]]\n",
            "\n",
            "W:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qr8gfZOqmSlC"
      },
      "source": [
        "To initiliaze all variables at once: use **`tf.global_variables_initializer()`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IFFDV5ujbjX9",
        "outputId": "bcfc03fc-0975-4227-d6fd-a9c40377ab2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  # Run initialization ops of all variables\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  print('s:')\n",
        "  print(sess.run(s))\n",
        "  print('\\nm:')\n",
        "  print(sess.run(m))\n",
        "  print('\\nW:')\n",
        "  print(sess.run(W))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s:\n",
            "2\n",
            "\n",
            "m:\n",
            "[[0 1]\n",
            " [2 3]]\n",
            "\n",
            "W:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Us76wF-aTaf"
      },
      "source": [
        "More encouraged way is using **`tf.get_variable`**, which allows us to provide the variable's internal name, shape, type, and initializer to give the variable its initial value.\n",
        "\n",
        "```\n",
        "tf.get_variable(\n",
        "    name,\n",
        "    shape=None,\n",
        "    dtype=None,\n",
        "    initializer=None,\n",
        "    regularizer=None,\n",
        "    trainable=True,\n",
        "    collections=None,\n",
        "    caching_device=None,\n",
        "    partitioner=None,\n",
        "    validate_shape=True,\n",
        "    use_resource=None,\n",
        "    custom_getter=None,\n",
        "    constraint=None\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "54ABBw1ljKqB"
      },
      "source": [
        "Create three variables using `tf.get_variable`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TV2kmEqUaeF8",
        "colab": {}
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  s = tf.get_variable('scalar', initializer=tf.constant(3))\n",
        "  m = tf.get_variable('matrix', initializer=tf.constant([[0,2], [1,3]]))\n",
        "  W = tf.get_variable('big_matrix', shape=(784, 10), initializer=tf.ones_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9iVhrjR-aptK",
        "outputId": "391747e1-c8dd-40e9-feb5-10eaa08d09c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "with tf.Session(graph=graph) as sess:\n",
        "  # Run initialization ops of all variables\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  print('s:')\n",
        "  print(sess.run(s))\n",
        "  print('\\nm:')\n",
        "  print(sess.run(m))\n",
        "  print('\\nW:')\n",
        "  print(sess.run(W))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s:\n",
            "3\n",
            "\n",
            "m:\n",
            "[[0 2]\n",
            " [1 3]]\n",
            "\n",
            "W:\n",
            "[[1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " ...\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]\n",
            " [1. 1. 1. ... 1. 1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "239nTB8ScEMD"
      },
      "source": [
        "### Changing values of variables\n",
        "\n",
        "To change the value of a variable, we need to assign a new value to the variable.\n",
        "You can see variable `v` changes after `assign` operations are executed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ezu6kFnScVd8",
        "outputId": "e09d30c1-c26c-4dd8-c194-38bf6d635f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # v is a 2 x 3 variable of random values\n",
        "  v = tf.get_variable('normal_matrix', shape=(2,3), initializer=tf.random_normal_initializer(mean=0., stddev=1.))\n",
        "  c = tf.constant(1.0, shape=(2,3))\n",
        "  assign_1 = v.assign(c)\n",
        "  assign_2 = v.assign([[1., 2., 3.], [4., 5., 6.]])\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  # Initialize variables\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  # or equivalently\n",
        "  # sess.run(v.initializer)\n",
        "  \n",
        "  # Get value\n",
        "  print('v:')\n",
        "  print(sess.run(v))\n",
        "\n",
        "  # Assign new value to the variable\n",
        "  sess.run(assign_1)\n",
        "\n",
        "  # Get value again\n",
        "  print('v:')\n",
        "  print(sess.run(v))\n",
        "\n",
        "  # Initialize the variable once again to random values\n",
        "  sess.run(v.initializer)\n",
        "\n",
        "  # Get value again\n",
        "  print('v:')\n",
        "  print(sess.run(v))\n",
        "\n",
        "  # Assign new value to the variable\n",
        "  sess.run(assign_2)\n",
        "\n",
        "  # Get value again\n",
        "  print('v:')\n",
        "  print(sess.run(v))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v:\n",
            "[[-1.6561947   0.34310102  0.708068  ]\n",
            " [-0.4922949   0.8811715  -0.31449178]]\n",
            "v:\n",
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "v:\n",
            "[[-1.4980592  -2.6887426  -0.60777825]\n",
            " [ 0.56541526  0.79404986 -0.48953685]]\n",
            "v:\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gs3yizqwmxHs"
      },
      "source": [
        "## Quiz 2\n",
        "Define a variable (name : \"term\") with shape = [] and dtype = `tf.float64`. Initialize the variable as `2` first.\n",
        "Define another variable (name : \"sum\") with shape = [] and dtype = `tf.float64`. Initialize the variable as zeros.\n",
        "\n",
        "By using these two variables, compute the following:\n",
        "sum = 1/term_1 + 1/term_2 + ... + 1/term_10\n",
        "where\n",
        "term_i = term_{i-1} * (term_{i-1} - 1) + 1\n",
        "and term_1 = 2.\n",
        "(This recurrence relation is known as Sylvester's sequence.)\n",
        "\n",
        "Hint: Repeat updating the variables \"sum\" and \"term\" 10 times.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rhqP7g40o0z2",
        "outputId": "24f201a9-6913-422b-970c-7e88325b4f94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ############# Write here. ################\n",
        "  t = tf.get_variable('term', dtype=tf.float64, initializer=tf.constant(2., dtype=tf.float64))\n",
        "  s = tf.get_variable('sum', dtype=tf.float64, initializer=tf.constant(0., dtype=tf.float64))\n",
        "  update_t = t.assign(t * (t - 1) + 1)\n",
        "  update_s = s.assign_add(1 / t)\n",
        "  ##########################################\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  ############# Write here. ################\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for _ in range(10):\n",
        "    sess.run(update_s)\n",
        "    sess.run(update_t)\n",
        "  ##########################################\n",
        "\n",
        "  print('s:', sess.run(s))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s: 0.9999999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgFhwdh4kJlB"
      },
      "source": [
        "# 2. Feeding Input Data into a TensorFlow Graph\n",
        "\n",
        "How to feed data into a TensorFlow program?\n",
        "\n",
        "Use `tf.Placeholder` or\n",
        "\n",
        "**TensorFlow Dataset API (recommended for large-scale data)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JNTEqevyzz2X"
      },
      "source": [
        "## Placeholder\n",
        "\n",
        "TensorFlow's feed mechanism lets you inject data into any Tensor in a computation graph. Feed your data (usually a Numpy array) by passing the `feed_dict` argument to the `sess.run()` call.\n",
        "\n",
        "TensorFlow provides a **placeholder** operation that must be fed with data on execution. A placeholder exists solely to serve as the target of feeds. It is not initialized and contains no data. A placeholder generates an error if it is executed without a feed, so you won't forget to feed it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7l66JRS_03Xc",
        "outputId": "45339492-ec09-4d93-a85e-fe3f4aa5678d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # Create a constant with 1.\n",
        "  a = tf.constant(1)\n",
        "\n",
        "  # Create scalar placeholder with integer data type.\n",
        "  b = tf.placeholder(shape=[], dtype=tf.int32)\n",
        "\n",
        "  # Create add operation using the constant and placeholder.\n",
        "  c = tf.add(a, b)\n",
        "  \n",
        "  with tf.Session(graph=graph) as sess:\n",
        "    print('a:', sess.run(a))\n",
        "\n",
        "    # cannot run the graph since we didn't feed value to the placeholder\n",
        "    try:\n",
        "      print('b:', sess.run(b))\n",
        "    except Exception as e:\n",
        "      print(e.message)\n",
        "\n",
        "    # cannot run the graph since we didn't feed value to the placeholder\n",
        "    try:\n",
        "      print('c:', sess.run(c))\n",
        "    except Exception as e:\n",
        "      print(e.message)\n",
        "\n",
        "    # feed value 2 to b (c = a + b = 1 + 2 = 3)\n",
        "    print('c = 1 + 2 =', sess.run(c, feed_dict={b: 2}))\n",
        "\n",
        "    # we can also feed a\n",
        "    print('c = -1 + 2 =', sess.run(c, feed_dict={a: -1, b: 2}))\n",
        "\n",
        "    # we can also feed c: in this case, we don't have to feed b since we don't need it\n",
        "    print('c =', sess.run(c, feed_dict={c:0}))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a: 1\n",
            "You must feed a value for placeholder tensor 'Placeholder' with dtype int32\n",
            "\t [[node Placeholder (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n",
            "You must feed a value for placeholder tensor 'Placeholder' with dtype int32\n",
            "\t [[node Placeholder (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n",
            "c = 1 + 2 = 3\n",
            "c = -1 + 2 = 1\n",
            "c = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lJ3XeX_Gn4a3"
      },
      "source": [
        "## Quiz 3\n",
        "Define a variable (name: `sum`, shape: [2, 3]).\n",
        "Initialize the variable as zeros first.\n",
        "\n",
        "Drawing random samples from a uniform distribution using `np.random.uniform(size=(2,3))`.\n",
        "\n",
        "Accumulate the value of the sample to the variable 1200 times.\n",
        "Subtract `600.0` from the variable and divide the variable by `10.0` to approximate a draw from standard normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N6VmIhrin4a4",
        "outputId": "e37e9d9a-a61d-42c6-cbb8-0dfc3eb7c1a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ############# Write here. ################\n",
        "  sample = tf.placeholder(shape=(2,3), dtype=tf.float32)\n",
        "  s = tf.get_variable('sum', shape=(2,3), initializer=tf.zeros_initializer())\n",
        "  update_s = s.assign_add(sample)\n",
        "  normalized = (s - 600.) / 10.\n",
        "  ##########################################\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  ############# Write here. ################\n",
        "  for _ in range(1200):\n",
        "    sess.run(update_s, feed_dict={sample: np.random.uniform(size=(2,3))})\n",
        "  print('approximate standard normal:')\n",
        "  print(sess.run(normalized))\n",
        "  ##########################################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "approximate standard normal:\n",
            "[[ 0.25338134  0.06036988 -0.34400636]\n",
            " [ 0.8322388  -0.59349364 -0.11800537]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9mHYbZmS06zb"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The `tf.data` API is the most advanced API for writing TensorFlow input pipelines.\n",
        "\n",
        "It allows you to build complex pipelines by composing simple building blocks. \n",
        "\n",
        "Two main abstractions introduced by the `tf.data` API are:\n",
        "* `tf.data.Dataset`: contains a sequence of items (each item represents one or more `tf.Tensor`s)\n",
        "* `tf.data.Iterator`: provides interface to iterate through the dataset\n",
        "\n",
        "Users can create new Datasets from existing `tf.Tensor`s by using static methods like `Dataset.from_tensor_slices()`. \n",
        "\n",
        "For example, you can create a Dataset of string Tensors that represents input file names. \n",
        "\n",
        "Transformation of exisiting Datasets is another way of creating new dataset. \n",
        "\n",
        "TensorFlow provides frequently-used Dataset transformations such as `Dataset.batch` or `Dataset.shuffle` (please refer to https://www.tensorflow.org/api_docs/python/tf/data/Dataset). \n",
        "\n",
        "An Iterator is associated with a particular Dataset and we can retrieve the next element by executing an operation returned by `Iterator.get_next()`. \n",
        "\n",
        "This typically acts as an interface between your Dataset input pipline and your model.\n",
        "\n",
        "The simplest way to construct an Iterator is using `tf.compat.v1.data.make_one_shot_iterator(Dataset)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G2N-qVWB4VNW"
      },
      "source": [
        "### `tf.data.Dataset.from_tensor_slices()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "knXKv93E4drK"
      },
      "source": [
        "Creates a Dataset whose elements are slices of the given *python array* or *numpy array* or *tensors*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7lVbbwCfzGbx",
        "outputId": "5f8d0ffa-46c3-4b7c-fab7-f7ee076da77b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "import numpy as np\n",
        "arr = np.arange(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # Create a dataset from a numpy array\n",
        "  ds = tf.data.Dataset.from_tensor_slices(arr)\n",
        "  \n",
        "  # Create an iterator for the dataset\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  \n",
        "  # Retrieve next element from theiterator\n",
        "  data_getter = iterator.get_next()\n",
        "  \n",
        "  # Multiply by 2\n",
        "  mul_2 = data_getter * 2\n",
        "  \n",
        "  with tf.Session(graph=graph) as sess:\n",
        "    for i in range(10):\n",
        "      print(sess.run(mul_2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "16\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hV-LVSuslOLE"
      },
      "source": [
        "## Create a dataset from files using the Dataset API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_ifr21zdal4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_and_print(iterator, graph, count=6):\n",
        "  with tf.Session(graph=graph) as sess:\n",
        "    # Read the first `count` elements of the iterator.\n",
        "    for i in range(count):\n",
        "      v = sess.run(iterator)\n",
        "      print('step %d, data: %s' % (i, v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mCt7x4ByogNm"
      },
      "source": [
        "### Create dummy binary files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MTFhI2kBomkO",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def create_bin_file(file_name, value):\n",
        "  with open(file_name, 'wb') as f:\n",
        "    f.write(np.arange(value, value+4, dtype=np.int32))\n",
        "    \n",
        "bin_filenames = []\n",
        "for i in range(3):\n",
        "  file_name = 'binary_file_%d'% i\n",
        "  create_bin_file(file_name, i)\n",
        "  bin_filenames.append(file_name)\n",
        "\n",
        "# first file:\n",
        "# 0 1 2 3\n",
        "\n",
        "# second file:\n",
        "# 1 2 3 4\n",
        "\n",
        "# third file:\n",
        "# 2 3 4 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0rBvsoONqBSl"
      },
      "source": [
        "### FixedLengthRecordDataset : each fixed-length slice of bytes is a dataset element.\n",
        "\n",
        "In this example, each data instance is a 8-byte integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "48JvZOGtqEwY",
        "outputId": "836bd46a-60d4-4be2-b4dc-1f00eb8904cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # create a Dataset that contains slices (size: 8 bytes) of the files\n",
        "  ds = tf.data.FixedLengthRecordDataset(bin_filenames, 8)\n",
        "  # or equivalently,\n",
        "  # ds = tf.data.Dataset.from_tensor_slices(bin_filenames)\n",
        "  # ds = ds.apply(lambda filename: tf.data.FixedLengthRecordDataset(filename, 8))\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  # contains raw byte strings\n",
        "  iterator = iterator.get_next()\n",
        "  # convert 8 bytes into int32 => two int32 value per each element\n",
        "  to_int = tf.io.decode_raw(iterator, 'int32')\n",
        "\n",
        "iterate_and_print(to_int, graph)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: [0 1]\n",
            "step 1, data: [2 3]\n",
            "step 2, data: [1 2]\n",
            "step 3, data: [3 4]\n",
            "step 4, data: [2 3]\n",
            "step 5, data: [4 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L5FwnpyyneZQ"
      },
      "source": [
        "### Create dummy text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BdSZO50jlSbB",
        "colab": {}
      },
      "source": [
        "def create_text_file(file_name, index):\n",
        "  with open(file_name, 'w') as f:\n",
        "    f.write('Hello_%d\\n' % index)\n",
        "    f.write('TensorFlow_%d\\n' % index)\n",
        "\n",
        "text_filenames = []\n",
        "for i in range(3):\n",
        "  file_name = 'text_file_%d'% i\n",
        "  create_text_file(file_name, i)\n",
        "  text_filenames.append(file_name)\n",
        "\n",
        "# first file:\n",
        "# Hello_0\n",
        "# TensorFlow_0\n",
        "\n",
        "# second file:\n",
        "# Hello_1\n",
        "# TensorFlow_1\n",
        "\n",
        "# third file:\n",
        "# Hello_2\n",
        "# TensorFlow_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jv2Vc8gt9WbO"
      },
      "source": [
        "### TextLineDataset : each text line is a dataset element.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hYY0TdipmvoU",
        "outputId": "d309acd3-d772-4e68-8b12-14559759750f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  # create a Dataset that contains each line of the text files\n",
        "  ds = tf.data.TextLineDataset(text_filenames)\n",
        "  # or equivalently,\n",
        "  # ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "  # ds = ds.apply(lambda filename: tf.data.TextLineDataset(filename))\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S8eqeLjir-ar"
      },
      "source": [
        "## Transform dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GinK40Kp5jQA"
      },
      "source": [
        "**ds.shffule(buffer_size)**\n",
        "\n",
        "shuffle: shuffle data instances randomly. buffer size represents the number of data instances to be sampled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CzImNht48XKW"
      },
      "source": [
        "`ds.shuffle` with N > 1 can pick data instances randomly from the buffer containing N instances. The code snippet below shows that we always do not get the 5th or 6th element of the dataset (Hello_2 or TensorFlow_2) at step 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b3oaSOHVsGLc",
        "outputId": "46935162-cce9-45c5-e2fd-66ff055f2ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames) \n",
        "  # shuffle the dataset using buffer size 4\n",
        "  ds = ds.shuffle(4)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_1'\n",
            "step 1, data: b'Hello_0'\n",
            "step 2, data: b'Hello_2'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'TensorFlow_0'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bP-MH37I8s8z"
      },
      "source": [
        "`ds.shuffle` with N == 1 has no shuffling effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c8Q9W2pG6nxF",
        "outputId": "dd73f1f5-b9cd-4463-8cb2-3208957bd998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames)\n",
        "  # shuffle the dataset using buffer size 1\n",
        "  ds = ds.shuffle(1)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9i-rJwnE6Vdj"
      },
      "source": [
        "**ds.repeat(count)**\n",
        "\n",
        "Repeat the data instances count times. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WM9xpfjr72Qo"
      },
      "source": [
        "Without `ds.repeat()`, an error is raised after reading all the data from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUzyWsUt6WQ_",
        "colab": {}
      },
      "source": [
        "# graph = tf.Graph()\n",
        "# with graph.as_default():\n",
        "#   ds = tf.data.TextLineDataset(text_filenames)\n",
        "\n",
        "#   iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "#   iterator = iterator.get_next()\n",
        "\n",
        "# iterate_and_print(iterator, graph, count=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z_CwPo5I76G9"
      },
      "source": [
        "`ds.repeat(count)` repeats iterating the dataset `count` times. If we do not pass `count` argument, we can iterate forever."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FzqJY23X8Hg0",
        "outputId": "8b8f822a-06d5-4051-9cbc-3f1cdf90a533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames)\n",
        "  # repeat twice\n",
        "  ds = ds.repeat(2)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph, count=12)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n",
            "step 6, data: b'Hello_0'\n",
            "step 7, data: b'TensorFlow_0'\n",
            "step 8, data: b'Hello_1'\n",
            "step 9, data: b'TensorFlow_1'\n",
            "step 10, data: b'Hello_2'\n",
            "step 11, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yM9UYyhdamR",
        "colab_type": "text"
      },
      "source": [
        "Another common pattern is to use try-except clause to detect the end of epoch. Once we finisn an epoch, we re-initialize the iterator to start from the beginning again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyHGXUxadamS",
        "colab_type": "code",
        "outputId": "395031ba-2a1f-4d26-cc2b-3585b8eab979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames)\n",
        "\n",
        "  # create an empty iterator with only type and shape information\n",
        "  iterator = tf.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(ds), tf.compat.v1.data.get_output_shapes(ds))\n",
        "\n",
        "  # create an operation for initializing the iterator with the dataset\n",
        "  init_iterator = iterator.make_initializer(ds)\n",
        "\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "epoch = 0\n",
        "step = 0\n",
        "with tf.Session(graph=graph) as sess:\n",
        "  # initialize the iterator\n",
        "  sess.run(init_iterator)\n",
        "  \n",
        "  while True:\n",
        "    # repeat until we detect an error\n",
        "    try:\n",
        "      v = sess.run(iterator)\n",
        "      print('step %d, data: %s' % (step, v))\n",
        "      step += 1\n",
        "    # iterator raises tf.errors.OutOfRangeError once we finish an epoch\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      print('Finished epoch', epoch)\n",
        "      epoch += 1\n",
        "      # if we are done with 2 epochs, break\n",
        "      if epoch >= 2:\n",
        "        break\n",
        "      # otherwise, re-initialize the iterator\n",
        "      sess.run(init_iterator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:347: Iterator.output_types (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:348: Iterator.output_shapes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(iterator)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py:350: Iterator.output_classes (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_classes(iterator)`.\n",
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n",
            "Finished epoch 0\n",
            "step 6, data: b'Hello_0'\n",
            "step 7, data: b'TensorFlow_0'\n",
            "step 8, data: b'Hello_1'\n",
            "step 9, data: b'TensorFlow_1'\n",
            "step 10, data: b'Hello_2'\n",
            "step 11, data: b'TensorFlow_2'\n",
            "Finished epoch 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "71CqxItI4_Rk"
      },
      "source": [
        "**ds.batch(batch_size)**\n",
        "\n",
        "Combines elements of this dataset into batches. `batch_size` represents the number of data instances to combine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8stEmpyxvT18",
        "outputId": "a6b1ceb6-ce75-428b-f5b6-5aa06d33581e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames) \n",
        "  # batch elements using batch_size 3\n",
        "  ds = ds.batch(3)\n",
        "  ds = ds.repeat(3)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
            "step 1, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n",
            "step 2, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
            "step 3, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n",
            "step 4, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
            "step 5, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "meX0yDAd4KCD"
      },
      "source": [
        "**ds.map(fn)**\n",
        "\n",
        "Apply `fn` to each element of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zITLEwgys7l8",
        "outputId": "16035bdb-da39-4225-b5b5-cf81a5589f7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# split the `data` tensor into 3 pieces and concatenate the pieces by inserting '+' between them\n",
        "def split_join(data):\n",
        "  data = tf.split(data, 3)\n",
        "  return tf.strings.join(data, '+')\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.TextLineDataset(text_filenames)\n",
        "  ds = ds.batch(3)\n",
        "  ds = ds.repeat(3)\n",
        "  ds = ds.map(split_join)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
            "step 1, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n",
            "step 2, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
            "step 3, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n",
            "step 4, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
            "step 5, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gXqUUZeo5a9E"
      },
      "source": [
        "##  Speed up Dataset processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WplDJMqQ59im"
      },
      "source": [
        "\n",
        "**ds.interleave(map_func, cycle_length)**\n",
        "\n",
        "map_func : map function to apply to each data instance\n",
        "\n",
        "cycle_length : the number of data instances to process concurrently"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "elNSpQlF9Kkd"
      },
      "source": [
        "We can use this feature to read and process multiple files concurrently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dlIOImd6KHHr",
        "outputId": "63f7dd1e-4167-4628-c1e9-bcfb984eb63f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "  # consume the first two files in concurrently, and then the third file \n",
        "  ds = ds.interleave(lambda filename: tf.data.TextLineDataset(filename),\n",
        "                     cycle_length=2)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'Hello_1'\n",
            "step 2, data: b'TensorFlow_0'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mhp0R08AKxpy"
      },
      "source": [
        "**ds.prefetch(buffer_size)** \n",
        "\n",
        "prefetch elements from a dataset. buffer size represents the maximum buffer size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AWM2PH6SLHrb",
        "outputId": "36f66235-ff13-4133-c449-359c48059e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "  ds = ds.interleave(lambda filename: tf.data.TextLineDataset(filename),\n",
        "                     cycle_length=3)\n",
        "  ds = ds.prefetch(3)\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'Hello_1'\n",
            "step 2, data: b'Hello_2'\n",
            "step 3, data: b'TensorFlow_0'\n",
            "step 4, data: b'TensorFlow_1'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fEs3qEJszYgu"
      },
      "source": [
        "## Quiz 4\n",
        "Create a dataset following the instructions.\n",
        "\n",
        "1. Create a textline dataset using files named `ex_filenames`. \n",
        "2. Shuffle the dataset with buffer size 15.\n",
        "3. Repeat the dataset for 2 epochs.\n",
        "4. Convert each data instance using the `cast` function defined below.\n",
        "5. Make the data instances as a batch (batch size = 3).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C_TOoWhx1O2S",
        "outputId": "1cedee64-c986-4009-cbde-74f856999f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def create_text_file(index):\n",
        "  with open('ex_file_%d'%index, 'w') as f:\n",
        "    for i in range(3):\n",
        "      f.write('%d.%d\\n' % (index, i))\n",
        "    \n",
        "ex_filenames = []\n",
        "for i in range(5):\n",
        "  create_text_file(i)\n",
        "  ex_filenames.append('ex_file_%d'% i)\n",
        "\n",
        "def cast(data):\n",
        "  data = tf.strings.to_number(data, out_type=tf.float32)\n",
        "  return data\n",
        "\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  ############# Write here. ################\n",
        "  ds = tf.data.TextLineDataset(ex_filenames)\n",
        "  ds = ds.shuffle(10)\n",
        "  ds = ds.repeat(2)\n",
        "  ds = ds.map(cast)\n",
        "  ds = ds.batch(3)\n",
        "  ##########################################\n",
        "\n",
        "  iterator = tf.compat.v1.data.make_one_shot_iterator(ds)\n",
        "  iterator = iterator.get_next()\n",
        "\n",
        "iterate_and_print(iterator, graph, count=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: [0.2 1.  2.2]\n",
            "step 1, data: [2.1 3.1 0. ]\n",
            "step 2, data: [4.  2.  1.2]\n",
            "step 3, data: [1.1 4.1 4.2]\n",
            "step 4, data: [3.2 0.1 3. ]\n",
            "step 5, data: [3.  2.1 2.2]\n",
            "step 6, data: [2.  0.1 4.1]\n",
            "step 7, data: [1.2 4.  1. ]\n",
            "step 8, data: [1.1 3.2 0. ]\n",
            "step 9, data: [3.1 0.2 4.2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z8oia-fDJ4cx"
      },
      "source": [
        "# 3. Logistic Regression\n",
        "\n",
        "Now, we can build a logistic regression example on TensorFlow by following the steps below.\n",
        "\n",
        "**1. Read MNIST data using the dataset API**\n",
        "\n",
        "**2. Create weights and biases**\n",
        "\n",
        "**3. Build a logistic regression model**\n",
        "\n",
        "**4. Define a loss function**\n",
        "\n",
        "**5. Define a training op**\n",
        "\n",
        "**6. Train and calculate accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WYPbCvJxL8fH"
      },
      "source": [
        "## Read MNIST data using the dataset API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86XRt-c9damj",
        "colab_type": "code",
        "outputId": "7e995e30-e9e2-468b-f77c-bf992beb7287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import gzip\n",
        "import os\n",
        "import sys\n",
        "import urllib\n",
        "\n",
        "try:\n",
        "    from urllib.error import URLError\n",
        "    from urllib.request import urlretrieve\n",
        "except ImportError:\n",
        "    from urllib2 import URLError\n",
        "    from urllib import urlretrieve\n",
        "\n",
        "RESOURCES = [\n",
        "    'train-images-idx3-ubyte.gz',\n",
        "    'train-labels-idx1-ubyte.gz',\n",
        "    't10k-images-idx3-ubyte.gz',\n",
        "    't10k-labels-idx1-ubyte.gz',\n",
        "]\n",
        "\n",
        "\n",
        "def report_download_progress(chunk_number, chunk_size, file_size):\n",
        "    if file_size != -1:\n",
        "        percent = min(1, (chunk_number * chunk_size) / file_size)\n",
        "        bar = '#' * int(64 * percent)\n",
        "        sys.stdout.write('\\r0% |{:<64}| {}%'.format(bar, int(percent * 100)))\n",
        "\n",
        "\n",
        "def download(destination_path, url, quiet):\n",
        "    if os.path.exists(destination_path):\n",
        "        if not quiet:\n",
        "            print('{} already exists, skipping ...'.format(destination_path))\n",
        "    else:\n",
        "        print('Downloading {} ...'.format(url))\n",
        "        try:\n",
        "            hook = None if quiet else report_download_progress\n",
        "            urlretrieve(url, destination_path, reporthook=hook)\n",
        "        except URLError:\n",
        "            raise RuntimeError('Error downloading resource!')\n",
        "        finally:\n",
        "            if not quiet:\n",
        "                # Just a newline.\n",
        "                print()\n",
        "\n",
        "\n",
        "def unzip(zipped_path, quiet):\n",
        "    unzipped_path = os.path.splitext(zipped_path)[0]\n",
        "    if os.path.exists(unzipped_path):\n",
        "        if not quiet:\n",
        "            print('{} already exists, skipping ... '.format(unzipped_path))\n",
        "        return\n",
        "    with gzip.open(zipped_path, 'rb') as zipped_file:\n",
        "        with open(unzipped_path, 'wb') as unzipped_file:\n",
        "            unzipped_file.write(zipped_file.read())\n",
        "            if not quiet:\n",
        "                print('Unzipped {}!'.format(zipped_path))\n",
        "\n",
        "\n",
        "mnist_dir = 'data/mnist'\n",
        "if not os.path.exists(mnist_dir):\n",
        "    os.makedirs(mnist_dir)\n",
        "\n",
        "\n",
        "for resource in RESOURCES:\n",
        "    path = os.path.join(mnist_dir, resource)\n",
        "    url = 'http://yann.lecun.com/exdb/mnist/{}'.format(resource)\n",
        "    download(path, url, False)\n",
        "    unzip(path, False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz ...\n",
            "0% |################################################################| 100%\n",
            "Unzipped data/mnist/train-images-idx3-ubyte.gz!\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz ...\n",
            "0% |################################################################| 100%\n",
            "Unzipped data/mnist/train-labels-idx1-ubyte.gz!\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz ...\n",
            "0% |################################################################| 100%\n",
            "Unzipped data/mnist/t10k-images-idx3-ubyte.gz!\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz ...\n",
            "0% |################################################################| 100%\n",
            "Unzipped data/mnist/t10k-labels-idx1-ubyte.gz!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "m_Pt_YweMFTP",
        "outputId": "72d7b6a0-e79e-452c-d4a9-72c473763b04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import struct\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Parse each MNIST data instance as images and labels\n",
        "def parse_data(path, dataset, flatten):\n",
        "    if dataset != 'train' and dataset != 't10k':\n",
        "        raise NameError('dataset must be train or t10k')\n",
        "\n",
        "    label_file = os.path.join(path, dataset + '-labels-idx1-ubyte')\n",
        "    with open(label_file, 'rb') as file:\n",
        "        _, num = struct.unpack(\">II\", file.read(8))\n",
        "        labels = np.fromfile(file, dtype=np.int8) #int8\n",
        "        new_labels = np.zeros((num, 10))\n",
        "        new_labels[np.arange(num), labels] = 1\n",
        "    \n",
        "    img_file = os.path.join(path, dataset + '-images-idx3-ubyte')\n",
        "    with open(img_file, 'rb') as file:\n",
        "        _, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "        imgs = np.fromfile(file, dtype=np.uint8).reshape(num, rows, cols) #uint8\n",
        "        imgs = imgs.astype(np.float32) / 255.0\n",
        "        if flatten:\n",
        "            imgs = imgs.reshape([num, -1])\n",
        "\n",
        "    return imgs, new_labels\n",
        "  \n",
        "# Read in the mnist dataset, given that the data is stored in path\n",
        "# Return two tuples of numpy arrays\n",
        "# ((train_imgs, train_labels), (test_imgs, test_labels))\n",
        "def read_mnist(path, flatten=True):\n",
        "    train = parse_data(path, 'train', flatten)\n",
        "    test = parse_data(path, 't10k', flatten)\n",
        "    return train, test\n",
        "\n",
        "mnist_dir = \"data/mnist\"\n",
        "train, test = read_mnist(mnist_dir, flatten=True)\n",
        "\n",
        "print(train[0].shape) # image\n",
        "print(train[1].shape) # label\n",
        "print(test[0].shape) # image\n",
        "print(test[1].shape) # label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000, 10)\n",
            "(10000, 784)\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lq_2ypB720KU"
      },
      "source": [
        "### Build an input pipeline using the dataset API\n",
        "\n",
        "Create datasets from numpy arrays that contain MNIST data.\n",
        "Then, batch them using batch size of 128."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N1-_jKWwNGyC",
        "colab": {}
      },
      "source": [
        "train, test = read_mnist(mnist_dir, flatten=True)\n",
        "batch_size = 128\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "  train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "  train_data = train_data.shuffle(10000)\n",
        "  train_data = train_data.batch(batch_size)\n",
        "\n",
        "  test_data = tf.data.Dataset.from_tensor_slices(test)\n",
        "  test_data = test_data.batch(batch_size)\n",
        "\n",
        "\n",
        "\n",
        "  # create an empty iterator with only type and shape information\n",
        "  iterator = tf.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(train_data),\n",
        "                                             tf.compat.v1.data.get_output_shapes(train_data))\n",
        "  image, label = iterator.get_next()\n",
        "\n",
        "  # create an operation for initializing the iterator with the train or test dataset\n",
        "  train_init = iterator.make_initializer(train_data)\n",
        "  test_init = iterator.make_initializer(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8H5Yn3pxNVOH"
      },
      "source": [
        "## Build a logistic regression model\n",
        "\n",
        "`w` is initialized to random normal distribution variables with mean 0 and standard deviation 0.01.\n",
        "`b` is initialized to 0's.\n",
        "The shape of `w` depends on the dimensions of `image` and `label` so that `label = tf.matmul(image, w)`.\n",
        "The shape of `b` depends on `label`.\n",
        "The cross entropy of softmax of logits is our loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PKOmF9JfNZLc",
        "colab": {}
      },
      "source": [
        "with graph.as_default():\n",
        "  w = tf.get_variable(name='weights_dataset', shape=(784, 10), initializer=tf.random_normal_initializer(0, 0.01))\n",
        "  b = tf.get_variable(name='bias_dataset', shape=(1, 10), initializer=tf.zeros_initializer())\n",
        "  logits = tf.matmul(image, w) + b\n",
        "  loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=label)\n",
        "  loss = tf.reduce_mean(loss) # average over all the examples in the batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nZDF4B7jN5r9"
      },
      "source": [
        "## Define a training op\n",
        "\n",
        "We'll use an Adam optimizer with a learning rate of 0.001 to minimize loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hGPK-eWzN4yc",
        "colab": {}
      },
      "source": [
        "with graph.as_default():\n",
        "  optimizer = tf.train.AdamOptimizer(0.001)\n",
        "  train_op = optimizer.minimize(loss)\n",
        "  preds = tf.nn.softmax(logits)\n",
        "  num_corrects = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))\n",
        "  num_corrects = tf.reduce_sum(tf.cast(num_corrects, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0BjGaGU4OM_O"
      },
      "source": [
        "## Train and calculate accuracy\n",
        "\n",
        "Finally, we train the model and use the test set to calculate the accuracy of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mfLm2JTjOOyy",
        "outputId": "b06d27c3-47f9-48de-c6e5-63ae83cd0174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "with tf.Session(graph=graph) as sess:  \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # train the model for 10 epochs\n",
        "    for i in range(10):\n",
        "        sess.run(train_init) # initialize the iterator using training set\n",
        "        total_loss = 0\n",
        "        n_batches = 0\n",
        "        try:\n",
        "            while True:\n",
        "                _, l = sess.run([train_op, loss])\n",
        "                total_loss += l\n",
        "                n_batches += 1\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        print('Epoch {0}: {1}'.format(i, total_loss/n_batches))   \n",
        "\n",
        "        # test the model\n",
        "        sess.run(test_init) # initialize the iterator using test set\n",
        "        total_num_corrects = 0\n",
        "        try:\n",
        "            while True: \n",
        "                total_num_corrects += sess.run(num_corrects)\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            pass\n",
        "        print('Accuracy {0}'.format(total_num_corrects/10000))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: 0.6618076512045952\n",
            "Accuracy 0.9016\n",
            "Epoch 1: 0.35990982335894856\n",
            "Accuracy 0.9116\n",
            "Epoch 2: 0.31811390305632975\n",
            "Accuracy 0.9175\n",
            "Epoch 3: 0.29899842659039283\n",
            "Accuracy 0.921\n",
            "Epoch 4: 0.2871422721887194\n",
            "Accuracy 0.9224\n",
            "Epoch 5: 0.2789779175509776\n",
            "Accuracy 0.9233\n",
            "Epoch 6: 0.2731518240879848\n",
            "Accuracy 0.9223\n",
            "Epoch 7: 0.26868408991456794\n",
            "Accuracy 0.9241\n",
            "Epoch 8: 0.2651266718247552\n",
            "Accuracy 0.9254\n",
            "Epoch 9: 0.26204027306995414\n",
            "Accuracy 0.9263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40YQKKM1damw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}